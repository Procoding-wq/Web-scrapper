This project is a Python-based desktop application that provides a simple GUI for extracting all URLs from any webpage. It is built using Tkinter for the interface and Selenium for handling dynamic content such as cookie banners or consent popups that often appear when visiting modern websites. The user simply enters a webpage URL into the app, clicks the "Fetch URLs" button, and the program launches a headless Chrome browser in the background to load the page, automatically attempt to dismiss any common cookie banners or popups, and then collect all the links (<a href> values) found on that page. The extracted URLs are displayed in a scrollable text box inside the application, allowing the user to copy or review them easily. Installation is straightforward: clone the repository, install the dependencies listed in requirements.txt (specifically selenium and webdriver-manager), and run the Python script to launch the app. Since tkinter comes preinstalled with Python, no extra installation is needed for the GUI. This tool is designed for convenience and demonstration purposes and only fetches links from the single page entered by the user; it does not crawl the entire website. For larger or more dynamic sites, the script could be extended with scrolling or crawling functionality. Users should also respect each websiteâ€™s Terms of Service and robots.txt rules when scraping or automating content extraction.
